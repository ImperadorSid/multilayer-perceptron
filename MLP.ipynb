{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#     self.inputs = [.05, .1]\n",
    "#     self.desired = [.01, .99]\n",
    "#     self.learning_rate = 0.5\n",
    "#     self.layers = [[Neuron(.05), Neuron(.1), Neuron(1)]]\n",
    "#     self.layers += [[Neuron([e.g for e in self.layers[0]], np.array([.15, .2, .35])), \n",
    "#                      Neuron([e.g for e in self.layers[0]], np.array([.25, .3, .35])), \n",
    "#                      Neuron(1)]]\n",
    "#     self.layers += [[Neuron([e.g for e in self.layers[1]], np.array([.4, .45, .6])), \n",
    "#                      Neuron([e.g for e in self.layers[1]], np.array([.5, .55, .6]))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     1,
     21
    ]
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, x=None, w=None, function=sigmoid):\n",
    "    self.x = x\n",
    "    self.u = None\n",
    "    self.pd = None\n",
    "    self.function = function\n",
    "    \n",
    "    if type(self.x) is float or type(self.x) is int:\n",
    "      # Bias ou entrada\n",
    "      self.w = None\n",
    "      self.g = self.x\n",
    "    elif type(self.x) is list:\n",
    "      # Normal\n",
    "      self.w = w if type(w) is np.ndarray else np.random.random(len(self.x))\n",
    "      self.u = self.x @ self.w\n",
    "      self.g = self.function(self.u)\n",
    "    else:\n",
    "      # Não-inicializado\n",
    "      self.w = w if w is None or type(w) is np.ndarray else np.random.random(w)\n",
    "      self.g = None\n",
    "      \n",
    "  def update(self, new_x):\n",
    "      self.x = new_x\n",
    "      if not self.is_bias():\n",
    "        self.u = self.x @ self.w\n",
    "        self.g = self.function(self.u)\n",
    "      else:\n",
    "        self.g = self.x\n",
    "  \n",
    "  is_bias = lambda self: True if self.w is None else False\n",
    "  \n",
    "  __repr__ = lambda self: '{} @ {} -> {}'.format(self.x, self.w, self.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "code_folding": [
     1,
     6,
     24,
     46,
     53,
     94
    ]
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, topology):\n",
    "    self.topology = topology\n",
    "    \n",
    "    self.init_neurons()   \n",
    "    \n",
    "  def init_neurons(self):\n",
    "    # Camada de entrada\n",
    "    self.layers = [[Neuron() for i in range(self.topology[0])]]\n",
    "    self.layers[0] += [Neuron(1)] # Bias \n",
    "    \n",
    "    # Camadas ocultas e de saída\n",
    "    for i in range(1, len(self.topology)):\n",
    "      self.layers += [[Neuron(w=len(self.layers[i - 1])) for j in range(self.topology[i])]]\n",
    "      if i != len(self.topology) - 1: self.layers[i] += [Neuron(1)] # Bias\n",
    "\n",
    "  def update_neurons(self): \n",
    "    # Camada de entrada\n",
    "    [self.layers[0][i].update(self.inputs[i]) for i in range(len(self.inputs))]\n",
    "    \n",
    "    # Camadas ocultas e de saída\n",
    "    for i in range(1, len(self.layers)):\n",
    "      [n.update([e.g for e in self.layers[i - 1]]) for n in self.layers[i] if not n.is_bias()]\n",
    "        \n",
    "  def update_weights(self):\n",
    "    # Camada de saída\n",
    "    for i in range(len(self.layers[-1])):\n",
    "      # Derivada parcial\n",
    "      neuron = self.layers[-1][i]\n",
    "      neuron.pd = - (self.desired[i] - neuron.g) * neuron.g * (1 - neuron.g)\n",
    "      # Pesos\n",
    "      for j in range(len(neuron.w)):\n",
    "        neuron.w[j] -= self.learning_rate * neuron.pd * self.layers[-2][j].g\n",
    "          \n",
    "    # Camadas ocultas\n",
    "    for i in range(len(self.layers) - 2, 0, -1):\n",
    "      for j in range(len(self.layers[i]) - 1):\n",
    "        # Derivada parcial\n",
    "        neuron = self.layers[i][j]\n",
    "#         print(neuron)\n",
    "        descendents = sum([n.pd * n.w[j] for n in self.layers[i + 1] if not n.is_bias()])\n",
    "        neuron.pd = descendents * neuron.g * (1 - neuron.g)\n",
    "        # Pesos\n",
    "        for k in range(len(neuron.w)):\n",
    "          neuron.w[k] -= self.learning_rate * neuron.pd * neuron.x[k]\n",
    "  \n",
    "  def update_errors(self):\n",
    "    out_layer = self.layers[-1]\n",
    "    out_length = len(out_layer)\n",
    "    \n",
    "    self.errors = [(out_layer[i].g - self.desired[i]) ** 2 for i in range(out_length)]\n",
    "    self.total_error = sum(self.errors) / 2\n",
    "    \n",
    "  def train(self, test_data, learning_rate=.1, precision=10**-6):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.precision = precision\n",
    "    self.epochs = 0\n",
    "    \n",
    "    # Erro inicial\n",
    "    errors_sum = 0\n",
    "    for index, sample in test_data.iterrows():\n",
    "      self.inputs = sample[:self.topology[0]].tolist()\n",
    "      self.desired = sample[self.topology[0]:].tolist()\n",
    "      self.update_neurons()\n",
    "\n",
    "      self.update_errors()\n",
    "      errors_sum += self.total_error\n",
    "    current_mse = errors_sum / test_data.shape[0]\n",
    "      \n",
    "    # Minimização dos erros  \n",
    "    while True:\n",
    "      last_mse = current_mse\n",
    "      \n",
    "      errors_sum = 0\n",
    "      for index, sample in test_data.iterrows():\n",
    "        self.inputs = sample[:self.topology[0]].tolist()\n",
    "        self.desired = sample[self.topology[0]:].tolist()\n",
    "        self.update_neurons()\n",
    "        \n",
    "        self.update_errors()\n",
    "        errors_sum += self.total_error\n",
    "        \n",
    "        self.update_weights()\n",
    "\n",
    "      self.update_neurons()\n",
    "      self.epochs += 1\n",
    "      \n",
    "      current_mse = errors_sum / test_data.shape[0]\n",
    "      print('{:4d} {:.9f} {:.9f} {:.9f}'.format(self.epochs, last_mse, current_mse, abs(current_mse -  last_mse)))\n",
    "      if abs(current_mse -  last_mse) <= self.precision: break\n",
    "        \n",
    "  def test_individual(self, input):\n",
    "    \n",
    "  \n",
    "  def __repr__(self):\n",
    "    result = 'Camada 0 - Entrada\\n{}\\n\\n'.format(self.layers[0])\n",
    "    for i in range(1, len(self.layers) - 1): result += 'Camada {} - Escondida \\n{}\\n\\n'.format(i, self.layers[i])\n",
    "    result += 'Camada {} - Saída\\n{}\\n\\n'.format(len(self.layers) - 1, self.layers[-1])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/ressonancia.csv')\n",
    "df = df.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 0.123484805 0.121924274 0.001560531\n",
      "   2 0.121924274 0.117058261 0.004866013\n",
      "   3 0.117058261 0.112082849 0.004975412\n",
      "   4 0.112082849 0.107019440 0.005063410\n",
      "   5 0.107019440 0.101892986 0.005126453\n",
      "   6 0.101892986 0.096731629 0.005161358\n",
      "   7 0.096731629 0.091566102 0.005165527\n",
      "   8 0.091566102 0.086428935 0.005137167\n",
      "   9 0.086428935 0.081353475 0.005075460\n",
      "  10 0.081353475 0.076372794 0.004980681\n",
      "  11 0.076372794 0.071518556 0.004854238\n",
      "  12 0.071518556 0.066819936 0.004698620\n",
      "  13 0.066819936 0.062302674 0.004517262\n",
      "  14 0.062302674 0.057988338 0.004314336\n",
      "  15 0.057988338 0.053893839 0.004094499\n",
      "  16 0.053893839 0.050031225 0.003862614\n",
      "  17 0.050031225 0.046407736 0.003623488\n",
      "  18 0.046407736 0.043026089 0.003381647\n",
      "  19 0.043026089 0.039884937 0.003141153\n",
      "  20 0.039884937 0.036979446 0.002905491\n",
      "  21 0.036979446 0.034301938 0.002677508\n",
      "  22 0.034301938 0.031842531 0.002459407\n",
      "  23 0.031842531 0.029589756 0.002252775\n",
      "  24 0.029589756 0.027531109 0.002058646\n",
      "  25 0.027531109 0.025653531 0.001877578\n",
      "  26 0.025653531 0.023943798 0.001709732\n",
      "  27 0.023943798 0.022388841 0.001554958\n",
      "  28 0.022388841 0.020975976 0.001412865\n",
      "  29 0.020975976 0.019693080 0.001282896\n",
      "  30 0.019693080 0.018528703 0.001164376\n",
      "  31 0.018528703 0.017472139 0.001056564\n",
      "  32 0.017472139 0.016513453 0.000958686\n",
      "  33 0.016513453 0.015643490 0.000869963\n",
      "  34 0.015643490 0.014853858 0.000789632\n",
      "  35 0.014853858 0.014136897 0.000716961\n",
      "  36 0.014136897 0.013485642 0.000651255\n",
      "  37 0.013485642 0.012893775 0.000591867\n",
      "  38 0.012893775 0.012355582 0.000538194\n",
      "  39 0.012355582 0.011865899 0.000489683\n",
      "  40 0.011865899 0.011420069 0.000445829\n",
      "  41 0.011420069 0.011013897 0.000406172\n",
      "  42 0.011013897 0.010643603 0.000370294\n",
      "  43 0.010643603 0.010305784 0.000337819\n",
      "  44 0.010305784 0.009997379 0.000308405\n",
      "  45 0.009997379 0.009715631 0.000281748\n",
      "  46 0.009715631 0.009458059 0.000257572\n",
      "  47 0.009458059 0.009222428 0.000235631\n",
      "  48 0.009222428 0.009006725 0.000215703\n",
      "  49 0.009006725 0.008809135 0.000197590\n",
      "  50 0.008809135 0.008628022 0.000181113\n",
      "  51 0.008628022 0.008461907 0.000166114\n",
      "  52 0.008461907 0.008309458 0.000152449\n",
      "  53 0.008309458 0.008169468 0.000139990\n",
      "  54 0.008169468 0.008040846 0.000128622\n",
      "  55 0.008040846 0.007922604 0.000118242\n",
      "  56 0.007922604 0.007813848 0.000108756\n",
      "  57 0.007813848 0.007713767 0.000100081\n",
      "  58 0.007713767 0.007621623 0.000092144\n",
      "  59 0.007621623 0.007536749 0.000084875\n",
      "  60 0.007536749 0.007458535 0.000078214\n",
      "  61 0.007458535 0.007386429 0.000072106\n",
      "  62 0.007386429 0.007319927 0.000066502\n",
      "  63 0.007319927 0.007258569 0.000061358\n",
      "  64 0.007258569 0.007201936 0.000056632\n",
      "  65 0.007201936 0.007149648 0.000052289\n",
      "  66 0.007149648 0.007101353 0.000048294\n",
      "  67 0.007101353 0.007056734 0.000044619\n",
      "  68 0.007056734 0.007015497 0.000041236\n",
      "  69 0.007015497 0.006977377 0.000038121\n",
      "  70 0.006977377 0.006942127 0.000035250\n",
      "  71 0.006942127 0.006909524 0.000032603\n",
      "  72 0.006909524 0.006879361 0.000030163\n",
      "  73 0.006879361 0.006851449 0.000027911\n",
      "  74 0.006851449 0.006825616 0.000025833\n",
      "  75 0.006825616 0.006801701 0.000023915\n",
      "  76 0.006801701 0.006779558 0.000022143\n",
      "  77 0.006779558 0.006759051 0.000020506\n",
      "  78 0.006759051 0.006740058 0.000018993\n",
      "  79 0.006740058 0.006722463 0.000017595\n",
      "  80 0.006722463 0.006706161 0.000016302\n",
      "  81 0.006706161 0.006691056 0.000015105\n",
      "  82 0.006691056 0.006677057 0.000013999\n",
      "  83 0.006677057 0.006664083 0.000012975\n",
      "  84 0.006664083 0.006652056 0.000012027\n",
      "  85 0.006652056 0.006640907 0.000011149\n",
      "  86 0.006640907 0.006630571 0.000010336\n",
      "  87 0.006630571 0.006620988 0.000009583\n",
      "  88 0.006620988 0.006612102 0.000008886\n",
      "  89 0.006612102 0.006603862 0.000008240\n",
      "  90 0.006603862 0.006596220 0.000007641\n",
      "  91 0.006596220 0.006589134 0.000007086\n",
      "  92 0.006589134 0.006582562 0.000006572\n",
      "  93 0.006582562 0.006576467 0.000006095\n",
      "  94 0.006576467 0.006570814 0.000005653\n",
      "  95 0.006570814 0.006565571 0.000005243\n",
      "  96 0.006565571 0.006560708 0.000004863\n",
      "  97 0.006560708 0.006556198 0.000004510\n",
      "  98 0.006556198 0.006552015 0.000004183\n",
      "  99 0.006552015 0.006548135 0.000003880\n",
      " 100 0.006548135 0.006544537 0.000003598\n",
      " 101 0.006544537 0.006541200 0.000003337\n",
      " 102 0.006541200 0.006538106 0.000003095\n",
      " 103 0.006538106 0.006535236 0.000002870\n",
      " 104 0.006535236 0.006532575 0.000002661\n",
      " 105 0.006532575 0.006530107 0.000002468\n",
      " 106 0.006530107 0.006527819 0.000002288\n",
      " 107 0.006527819 0.006525698 0.000002121\n",
      " 108 0.006525698 0.006523731 0.000001966\n",
      " 109 0.006523731 0.006521909 0.000001823\n",
      " 110 0.006521909 0.006520219 0.000001690\n",
      " 111 0.006520219 0.006518653 0.000001566\n",
      " 112 0.006518653 0.006517202 0.000001451\n",
      " 113 0.006517202 0.006515858 0.000001344\n",
      " 114 0.006515858 0.006514612 0.000001246\n",
      " 115 0.006514612 0.006513458 0.000001154\n",
      " 116 0.006513458 0.006512390 0.000001069\n",
      " 117 0.006512390 0.006511400 0.000000990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Camada 0 - Entrada\n",
       "[0.6796 @ None -> 0.6796, 0.4117 @ None -> 0.4117, 1 @ None -> 1]\n",
       "\n",
       "Camada 1 - Escondida \n",
       "[[0.6796, 0.4117, 1] @ [0.56029153 0.03714048 0.49353213] -> 0.7088069973818766, [0.6796, 0.4117, 1] @ [0.3328275  0.80731337 0.84477668] -> 0.8027129120083004, [0.6796, 0.4117, 1] @ [0.03684758 0.91394674 0.64014878] -> 0.7391320517763066, 1 @ None -> 1]\n",
       "\n",
       "Camada 2 - Escondida \n",
       "[[0.7088069973818766, 0.8027129120083004, 0.7391320517763066, 1] @ [0.96903565 0.73587105 0.13214993 0.84394597] -> 0.90195823620468, [0.7088069973818766, 0.8027129120083004, 0.7391320517763066, 1] @ [0.09459539 0.76900395 0.83386625 0.78286773] -> 0.8892934514065401, 1 @ None -> 1]\n",
       "\n",
       "Camada 3 - Saída\n",
       "[[0.90195823620468, 0.8892934514065401, 1] @ [-0.32360978 -0.56342688  0.08211836] -> 0.32941856428830285, [0.90195823620468, 0.8892934514065401, 1] @ [ 0.62064715 -0.08168467  0.42077761] -> 0.7125771641854513]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MLP([2, 3, 2, 2])\n",
    "a.train(df)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
