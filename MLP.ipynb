{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, x, w=[], function=sigmoid):\n",
    "    # function - Função de ativação\n",
    "    # x - Entrada\n",
    "    # w - Peso\n",
    "    # u - Saída do corpo\n",
    "    # g - Saída da função de ativação\n",
    "    # dp - Derivada parcial\n",
    "    \n",
    "    self.function = function\n",
    "    self.x = x\n",
    "    self.dp = None\n",
    "    \n",
    "    if type(x) is not list:\n",
    "      self.w = w\n",
    "      self.u = None\n",
    "      self.g = x\n",
    "    else:\n",
    "      self.w = np.random.random(len(self.x)) if w == [] else w\n",
    "      self.u = self.x @ self.w\n",
    "      self.g = function(self.u)\n",
    "  \n",
    "  def __repr__(self): return '{} @ {} -> {} -> {}'.format(self.x, self.w, self.u, self.g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "  def __init__(self, inputs, desired, topology, precision=10**-6):\n",
    "    self.inputs = inputs\n",
    "    self.desired = desired\n",
    "    self.topology = topology\n",
    "    self.precision = precision\n",
    "    self.errors = [0] * topology[-1]\n",
    "    self.total_error = 0\n",
    "    \n",
    "    # Camada de entrada\n",
    "    self.layers = [[Neuron(inputs[i]) for i in range(topology[0])]]\n",
    "    self.layers[0] += [Neuron(1)] # Bias \n",
    "    \n",
    "    # Camadas ocultas e de saída\n",
    "    for i in range(1, len(topology)):\n",
    "      self.layers += [[Neuron([e.g for e in self.layers[i - 1]]) for j in range(topology[i])]]\n",
    "      if i != len(topology) - 1: self.layers[i] += [Neuron(1)] # Bias\n",
    "\n",
    "#     self.layers = [[Neuron(.05), Neuron(.1), Neuron(1)]]\n",
    "#     self.layers += [[Neuron([e.g for e in self.layers[0]], np.array([.15, .2, .35])), \n",
    "#                      Neuron([e.g for e in self.layers[0]], np.array([.25, .3, .35])), \n",
    "#                      Neuron(1)]]\n",
    "#     self.layers += [[Neuron([e.g for e in self.layers[1]], np.array([.4, .45, .6])), \n",
    "#                      Neuron([e.g for e in self.layers[1]], np.array([.5, .55, .6]))]]\n",
    "      \n",
    "#     print(self.layers)\n",
    "    \n",
    "    self.update_errors()\n",
    "    \n",
    "  def update_errors(self):\n",
    "    out_layer = self.layers[-1]\n",
    "    out_length = len(out_layer)\n",
    "    \n",
    "    self.errors = [(out_layer[i].g - self.desired[i]) ** 2 / out_length for i in range(out_length)]\n",
    "    self.total_error = sum(self.errors)\n",
    "  \n",
    "  def update_weights(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2 @ [] -> None -> 2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " MLP([2, 1, 3],[0.3, 0.5, 0.5],[3, 10, 1]).layers[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
